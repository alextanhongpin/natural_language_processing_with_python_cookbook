{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Classification\n",
    "\n",
    "This example shows the classification of email using deep neural networks after generating tf-idf.\n",
    "\n",
    "We will use deep neural networks to classify email into one of the 20 pre-trained categories based on the words present in each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "x_train, y_train = newsgroups_train.data, newsgroups_train.target\n",
    "x_test, y_test = newsgroups_test.data, newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all 20 categories:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('List of all 20 categories:')\n",
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample email:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample email:\")\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample target category:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample target category:\")\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.autos'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.0-cp38-cp38-macosx_10_9_x86_64.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 3.7 MB/s eta 0:00:01    |████▎                           | 1.4 MB 3.7 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from pandas) (1.19.1)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.0 pytz-2020.1\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "!pip3 install pandas\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwds = set(stopwords.words('english'))\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = ' '.join(''.join([' ' if ch in string.punctuation else ch for ch in text]).split())\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "    noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "    verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def prat_lemmatize(token, tag):\n",
    "        \"\"\"resolve mismatch between pos_tag function and intake value of lemmatize function.\"\"\"\n",
    "        if tag in noun_tags:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "        elif tag in verb_tags:\n",
    "            return lemmatizer.lemmatize(token, 'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "    \n",
    "    pre_proc_text = ' '.join([prat_lemmatize(token, tag) for token, tag in tagged_corpus])\n",
    "    return pre_proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 11314 training data\n",
      "completed: 0\n",
      "completed: 100\n",
      "completed: 200\n",
      "completed: 300\n",
      "completed: 400\n",
      "completed: 500\n",
      "completed: 600\n",
      "completed: 700\n",
      "completed: 800\n",
      "completed: 900\n",
      "completed: 1000\n",
      "completed: 1100\n",
      "completed: 1200\n",
      "completed: 1300\n",
      "completed: 1400\n",
      "completed: 1500\n",
      "completed: 1600\n",
      "completed: 1700\n",
      "completed: 1800\n",
      "completed: 1900\n",
      "completed: 2000\n",
      "completed: 2100\n",
      "completed: 2200\n",
      "completed: 2300\n",
      "completed: 2400\n",
      "completed: 2500\n",
      "completed: 2600\n",
      "completed: 2700\n",
      "completed: 2800\n",
      "completed: 2900\n",
      "completed: 3000\n",
      "completed: 3100\n",
      "completed: 3200\n",
      "completed: 3300\n",
      "completed: 3400\n",
      "completed: 3500\n",
      "completed: 3600\n",
      "completed: 3700\n",
      "completed: 3800\n",
      "completed: 3900\n",
      "completed: 4000\n",
      "completed: 4100\n",
      "completed: 4200\n",
      "completed: 4300\n",
      "completed: 4400\n",
      "completed: 4500\n",
      "completed: 4600\n",
      "completed: 4700\n",
      "completed: 4800\n",
      "completed: 4900\n",
      "completed: 5000\n",
      "completed: 5100\n",
      "completed: 5200\n",
      "completed: 5300\n",
      "completed: 5400\n",
      "completed: 5500\n",
      "completed: 5600\n",
      "completed: 5700\n",
      "completed: 5800\n",
      "completed: 5900\n",
      "completed: 6000\n",
      "completed: 6100\n",
      "completed: 6200\n",
      "completed: 6300\n",
      "completed: 6400\n",
      "completed: 6500\n",
      "completed: 6600\n",
      "completed: 6700\n",
      "completed: 6800\n",
      "completed: 6900\n",
      "completed: 7000\n",
      "completed: 7100\n",
      "completed: 7200\n",
      "completed: 7300\n",
      "completed: 7400\n",
      "completed: 7500\n",
      "completed: 7600\n",
      "completed: 7700\n",
      "completed: 7800\n",
      "completed: 7900\n",
      "completed: 8000\n",
      "completed: 8100\n",
      "completed: 8200\n",
      "completed: 8300\n",
      "completed: 8400\n",
      "completed: 8500\n",
      "completed: 8600\n",
      "completed: 8700\n",
      "completed: 8800\n",
      "completed: 8900\n",
      "completed: 9000\n",
      "completed: 9100\n",
      "completed: 9200\n",
      "completed: 9300\n",
      "completed: 9400\n",
      "completed: 9500\n",
      "completed: 9600\n",
      "completed: 9700\n",
      "completed: 9800\n",
      "completed: 9900\n",
      "completed: 10000\n",
      "completed: 10100\n",
      "completed: 10200\n",
      "completed: 10300\n",
      "completed: 10400\n",
      "completed: 10500\n",
      "completed: 10600\n",
      "completed: 10700\n",
      "completed: 10800\n",
      "completed: 10900\n",
      "completed: 11000\n",
      "completed: 11100\n",
      "completed: 11200\n",
      "completed: 11300\n",
      "completed preprocessing training data\n",
      "processing 7532 testing data\n",
      "completed: 0\n",
      "completed: 100\n",
      "completed: 200\n",
      "completed: 300\n",
      "completed: 400\n",
      "completed: 500\n",
      "completed: 600\n",
      "completed: 700\n",
      "completed: 800\n",
      "completed: 900\n",
      "completed: 1000\n",
      "completed: 1100\n",
      "completed: 1200\n",
      "completed: 1300\n",
      "completed: 1400\n",
      "completed: 1500\n",
      "completed: 1600\n",
      "completed: 1700\n",
      "completed: 1800\n",
      "completed: 1900\n",
      "completed: 2000\n",
      "completed: 2100\n",
      "completed: 2200\n",
      "completed: 2300\n",
      "completed: 2400\n",
      "completed: 2500\n",
      "completed: 2600\n",
      "completed: 2700\n",
      "completed: 2800\n",
      "completed: 2900\n",
      "completed: 3000\n",
      "completed: 3100\n",
      "completed: 3200\n",
      "completed: 3300\n",
      "completed: 3400\n",
      "completed: 3500\n",
      "completed: 3600\n",
      "completed: 3700\n",
      "completed: 3800\n",
      "completed: 3900\n",
      "completed: 4000\n",
      "completed: 4100\n",
      "completed: 4200\n",
      "completed: 4300\n",
      "completed: 4400\n",
      "completed: 4500\n",
      "completed: 4600\n",
      "completed: 4700\n",
      "completed: 4800\n",
      "completed: 4900\n",
      "completed: 5000\n",
      "completed: 5100\n",
      "completed: 5200\n",
      "completed: 5300\n",
      "completed: 5400\n",
      "completed: 5500\n",
      "completed: 5600\n",
      "completed: 5700\n",
      "completed: 5800\n",
      "completed: 5900\n",
      "completed: 6000\n",
      "completed: 6100\n",
      "completed: 6200\n",
      "completed: 6300\n",
      "completed: 6400\n",
      "completed: 6500\n",
      "completed: 6600\n",
      "completed: 6700\n",
      "completed: 6800\n",
      "completed: 6900\n",
      "completed: 7000\n",
      "completed: 7100\n",
      "completed: 7200\n",
      "completed: 7300\n",
      "completed: 7400\n",
      "completed: 7500\n",
      "completed preprocessing testing data\n"
     ]
    }
   ],
   "source": [
    "print('processing {} training data'.format(len(x_train)))\n",
    "x_train_preprocessed = []\n",
    "for i, doc in enumerate(x_train):\n",
    "    x_train_preprocessed.append(preprocessing(doc))\n",
    "    if i % len(x_train)//10 == 0:\n",
    "        print('completed: {}'.format(i))\n",
    "print('completed preprocessing training data')\n",
    "        \n",
    "print('processing {} testing data'.format(len(x_test)))\n",
    "x_test_preprocessed = []\n",
    "for i, doc in enumerate(x_test):\n",
    "    x_test_preprocessed.append(preprocessing(doc))\n",
    "    if i % len(x_test)//10 == 0:\n",
    "        print('completed: {}'.format(i))\n",
    "print('completed preprocessing testing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building TFIDF vectorizer.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1,2), \n",
    "                            stop_words='english',\n",
    "                            max_features=10_000,\n",
    "                            strip_accents='unicode',\n",
    "                            norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (1.19.1)\n",
      "Requirement already satisfied: keras in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (2.4.3)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.0-cp38-cp38-macosx_10_11_x86_64.whl (165.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 165.2 MB 66 kB/s  eta 0:00:011  |█▉                              | 9.3 MB 3.5 MB/s eta 0:00:45     |████                            | 21.1 MB 3.0 MB/s eta 0:00:49     |█████▌                          | 28.5 MB 3.0 MB/s eta 0:00:47     |█████▉                          | 30.1 MB 9.6 MB/s eta 0:00:15     |██████████▌                     | 54.1 MB 48.9 MB/s eta 0:00:03     |████████████████████████▏       | 124.9 MB 8.6 MB/s eta 0:00:05     |████████████████████████▋       | 127.3 MB 8.6 MB/s eta 0:00:05     |████████████████████████████    | 144.6 MB 9.6 MB/s eta 0:00:03     |█████████████████████████████▊  | 153.6 MB 25.3 MB/s eta 0:00:01     |██████████████████████████████▉ | 159.4 MB 40.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from keras) (1.5.2)\n",
      "Requirement already satisfied: h5py in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from tensorflow) (0.34.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 25.4 MB/s eta 0:00:01\n",
      "\u001b[?25hProcessing /Users/alextanhongpin/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73/wrapt-1.12.1-cp38-cp38-macosx_10_15_x86_64.whl\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.12.4-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 40.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.30.0-cp38-cp38-macosx_10_9_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 34.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/alextanhongpin/.local/share/virtualenvs/natural_language_processing_with_python_co-tJ8vfo8Q/lib/python3.8/site-packages (from protobuf>=3.9.2->tensorflow) (49.2.0)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.20.0-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 4.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 20.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 22.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 8.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=e9d3fee2563366adbedf69cc142324a0280f035f2de1ad74ac87557397ba9838\n",
      "  Stored in directory: /Users/alextanhongpin/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=8f6d371bcdbcf64f24c130bbc30348bf5e2e1f8487dbd10b656e2fa717a9c2e3\n",
      "  Stored in directory: /Users/alextanhongpin/Library/Caches/pip/wheels/1d/10/8e/2f79b924179ff1e6510933d63eb851bea01054fff262343b7a\n",
      "Successfully built termcolor absl-py\n",
      "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.5.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: astunparse, tensorflow-estimator, gast, opt-einsum, termcolor, google-pasta, keras-preprocessing, absl-py, wrapt, protobuf, grpcio, chardet, urllib3, certifi, idna, requests, oauthlib, requests-oauthlib, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, werkzeug, markdown, tensorboard-plugin-wit, tensorboard, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 gast-0.3.3 google-auth-1.20.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 idna-2.10 keras-preprocessing-1.1.2 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.12.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 termcolor-1.1.0 urllib3-1.25.10 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "# Deep learning module.\n",
    "!pip3 install numpy keras tensorflow\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition hyper parameters\n",
    "np.random.seed(1337)\n",
    "nb_classes = 20\n",
    "batch_size = 64\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the 20 categories into one-hot encoding vectors in which 20 columns are created and the values\n",
    "# against the respective classes are given as 1. All other classes are given as 0.\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1000)              10001000  \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                25050     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 10,527,570\n",
      "Trainable params: 10,527,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Deep layer model building in keras.\n",
    "model = Sequential()\n",
    "model.add(Dense(1_000, input_shape=(10_000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "177/177 [==============================] - 11s 63ms/step - loss: 1.7257\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 11s 63ms/step - loss: 0.5363\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 11s 62ms/step - loss: 0.2874\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.1719\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 11s 63ms/step - loss: 0.1143\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 11s 61ms/step - loss: 0.0906\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 12s 68ms/step - loss: 0.0708\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 11s 60ms/step - loss: 0.0630\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 10s 57ms/step - loss: 0.0525\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 10s 59ms/step - loss: 0.0522\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 11s 62ms/step - loss: 0.0445\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 11s 64ms/step - loss: 0.0322\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 10s 58ms/step - loss: 0.0409\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 10s 58ms/step - loss: 0.0362\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 11s 61ms/step - loss: 0.0320\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 10s 58ms/step - loss: 0.0376\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 11s 60ms/step - loss: 0.0257\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 10s 58ms/step - loss: 0.0315\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 12s 67ms/step - loss: 0.0324\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 11s 65ms/step - loss: 0.0313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14dae5d90>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training.\n",
    "model.fit(x_train_2, y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction.\n",
    "y_train_predclass = model.predict(x_train_2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predclass = model.predict(x_test_2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural network - train accuracy: 0.999\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_result = np_utils.to_categorical(np.argmax(y_train_predclass, axis=-1), nb_classes)\n",
    "print('Deep neural network - train accuracy: {}'.format(round(accuracy_score(y_train, y_train_pred_result), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural network - test accuracy: 0.815\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_result = np.argmax(y_test_predclass, axis=-1)\n",
    "print('Deep neural network - test accuracy: {}'.format(round(accuracy_score(y_test, y_test_pred_result), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural network - train classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       480\n",
      "           1       1.00      1.00      1.00       584\n",
      "           2       1.00      1.00      1.00       591\n",
      "           3       1.00      1.00      1.00       590\n",
      "           4       1.00      1.00      1.00       578\n",
      "           5       1.00      1.00      1.00       593\n",
      "           6       1.00      1.00      1.00       585\n",
      "           7       1.00      1.00      1.00       594\n",
      "           8       1.00      1.00      1.00       598\n",
      "           9       1.00      1.00      1.00       597\n",
      "          10       1.00      1.00      1.00       600\n",
      "          11       1.00      1.00      1.00       595\n",
      "          12       1.00      1.00      1.00       591\n",
      "          13       1.00      1.00      1.00       594\n",
      "          14       1.00      1.00      1.00       593\n",
      "          15       1.00      1.00      1.00       599\n",
      "          16       1.00      1.00      1.00       546\n",
      "          17       1.00      1.00      1.00       564\n",
      "          18       1.00      1.00      1.00       465\n",
      "          19       1.00      1.00      1.00       377\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     11314\n",
      "   macro avg       1.00      1.00      1.00     11314\n",
      "weighted avg       1.00      1.00      1.00     11314\n",
      " samples avg       1.00      1.00      1.00     11314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Deep neural network - train classification report')\n",
    "print(classification_report(y_train, y_train_pred_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep neural network - test classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.76      0.80       319\n",
      "           1       0.68      0.75      0.71       389\n",
      "           2       0.77      0.65      0.71       394\n",
      "           3       0.63      0.73      0.68       392\n",
      "           4       0.73      0.81      0.77       385\n",
      "           5       0.84      0.76      0.80       395\n",
      "           6       0.81      0.83      0.82       390\n",
      "           7       0.87      0.86      0.87       396\n",
      "           8       0.91      0.93      0.92       398\n",
      "           9       0.86      0.93      0.89       397\n",
      "          10       0.94      0.96      0.95       399\n",
      "          11       0.94      0.88      0.91       396\n",
      "          12       0.75      0.69      0.72       393\n",
      "          13       0.89      0.83      0.86       396\n",
      "          14       0.87      0.91      0.89       394\n",
      "          15       0.86      0.89      0.87       398\n",
      "          16       0.72      0.84      0.77       364\n",
      "          17       0.97      0.88      0.92       376\n",
      "          18       0.76      0.63      0.69       310\n",
      "          19       0.65      0.67      0.66       251\n",
      "\n",
      "    accuracy                           0.81      7532\n",
      "   macro avg       0.81      0.81      0.81      7532\n",
      "weighted avg       0.82      0.81      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Deep neural network - test classification report')\n",
    "print(classification_report(y_test, y_test_pred_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
